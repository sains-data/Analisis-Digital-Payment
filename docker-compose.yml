version: '3.7'

services:
  namenode:
    image: apache/hadoop:3.4.1
    container_name: namenode
    restart: always
    user: "root"
    command: >
      bash -c "
        if [ ! -f /opt/hadoop/dfs/name/current/VERSION ]; then
          echo 'Formatting namenode...'
          hdfs namenode -format -force -nonInteractive
        fi
        echo 'Starting namenode...'
        hdfs namenode
      "
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - hadoop_namenode:/opt/hadoop/dfs/name
      - ./data:/data
      - ./config/hadoop:/opt/hadoop/etc/hadoop
      - ./prepare_hdfs_data.sh:/data/prepare_hdfs_data.sh
    environment:
      - CLUSTER_NAME=credit-card-cluster
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      retries: 3
      start_period: 60s
      timeout: 30s

  datanode:
    image: apache/hadoop:3.4.1
    container_name: datanode
    restart: always
    user: "root"
    command: ["hdfs", "datanode"]
    ports:
      - "9864:9864"
    volumes:
      - hadoop_datanode:/opt/hadoop/dfs/data
      - ./data:/data
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
    networks:
      - hadoop
    depends_on:
      - namenode
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9864"]
      interval: 30s
      retries: 3
      start_period: 30s
      timeout: 30s

  resourcemanager:
    image: apache/hadoop:3.4.1
    container_name: resourcemanager
    restart: always
    user: "root"
    command: ["yarn", "resourcemanager"]
    ports:
      - "8089:8089"  # Changed to 8089 to avoid conflict with Superset
    volumes:
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop
    depends_on:
      - namenode
      - datanode
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088"]
      interval: 30s
      retries: 3
      start_period: 30s
      timeout: 30s

  nodemanager:
    image: apache/hadoop:3.4.1
    container_name: nodemanager
    restart: always
    user: "root"
    command: ["yarn", "nodemanager"]
    ports:
      - "8042:8042"
    volumes:
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop
    depends_on:
      - resourcemanager
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8042"]
      interval: 30s
      retries: 3
      start_period: 30s
      timeout: 30s

  historyserver:
    image: apache/hadoop:3.4.1
    container_name: historyserver
    restart: always
    user: "root"
    command: ["mapred", "historyserver"]
    ports:
      - "8188:8188"
    volumes:
      - hadoop_historyserver:/opt/hadoop/logs/history
      - ./config/hadoop:/opt/hadoop/etc/hadoop
    environment:
      - MAPRED_HISTORYSERVER_USER=root
    networks:
      - hadoop
    depends_on:
      - resourcemanager
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188"]
      interval: 30s
      retries: 3
      start_period: 30s
      timeout: 30s

  spark-master:
    image: apache/spark:3.5.4-scala2.12-java11-python3-ubuntu
    container_name: spark-master
    restart: always
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "8081:8080"  # Changed from 8080:8080 to avoid conflict with Superset
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
      - ./config/spark:/opt/spark/conf
    networks:
      - hadoop
    depends_on:
      - namenode
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      retries: 3
      start_period: 30s
      timeout: 30s

  spark-worker-1:
    image: apache/spark:3.5.4-scala2.12-java11-python3-ubuntu
    container_name: spark-worker-1
    restart: always
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    ports:
      - "8082:8081"  # Changed to avoid conflicts
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=8881
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
      - ./config/spark:/opt/spark/conf
    networks:
      - hadoop
    depends_on:
      - spark-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      retries: 3
      start_period: 30s
      timeout: 30s

  jupyter:
    image: jupyter/all-spark-notebook:latest
    container_name: jupyter
    restart: always
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_PYTHON=/opt/conda/bin/python
      - PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data
      - ./config/spark:/opt/spark/conf
    networks:
      - hadoop
    depends_on:
      - spark-master  superset:
    image: apache/superset:latest
    container_name: superset
    restart: always
    ports:
      - "8088:8088"
    depends_on:
      - spark-master
    environment:
      - SUPERSET_SECRET_KEY=A_very_long_and_secure_secret_key_for_superset_that_is_at_least_42_characters_long_12345
      - SUPERSET_LOAD_EXAMPLES=false
      - PYTHONPATH=/app/pythonpath:/app/superset_init
      - WTF_CSRF_ENABLED=false
      - SUPERSET_WEBSERVER_ADDRESS=0.0.0.0    volumes:
      - superset_home:/app/superset_home
    command: >
      bash -c "
        superset db upgrade &&
        (superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin || echo 'Admin user may already exist') &&
        superset init &&
        gunicorn --bind 0.0.0.0:8088 --workers 2 --worker-class gthread --threads 20 --timeout 60 'superset.app:create_app()'
      "
    networks:
      - hadoop
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088"]
      interval: 30s
      retries: 3
      start_period: 60s
      timeout: 30s

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  superset_home:

networks:
  hadoop:
    driver: bridge
